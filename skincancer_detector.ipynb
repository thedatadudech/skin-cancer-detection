{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Skin Cancer Detection System - Complete Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thedatadudech/skin-cancer-detection/blob/main/skincancer_detector.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/thedatadudech/skin-cancer-detection)\n",
    "\n",
    "This comprehensive notebook demonstrates the complete development process of an advanced skin cancer detection system using deep learning with PyTorch. \n",
    "\n",
    "## üìã What You'll Learn\n",
    "- **Data Analysis**: Explore the HAM10000 medical imaging dataset\n",
    "- **Model Development**: Compare EfficientNet, ResNet, and custom CNN architectures\n",
    "- **Training Pipeline**: Implement robust training with data augmentation\n",
    "- **Performance Evaluation**: Analyze model performance across 7 skin lesion types\n",
    "- **Deployment Ready**: Export models for production use\n",
    "\n",
    "## üéØ Medical Context\n",
    "Early detection of skin cancer, particularly melanoma, is crucial for successful treatment. This system assists healthcare professionals in preliminary screening by analyzing dermoscopic images.\n",
    "\n",
    "**‚ö†Ô∏è Medical Disclaimer**: This tool is for educational and research purposes only. Always consult qualified healthcare professionals for medical diagnosis.\n",
    "\n",
    "## üöÄ Quick Start\n",
    "1. Click the \"Open in Colab\" button above\n",
    "2. Run the setup cells to install dependencies\n",
    "3. Download the HAM10000 dataset (instructions below)\n",
    "4. Execute the training pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Google Colab Setup\n",
    "\n",
    "If running on Google Colab, execute the following cells to set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for Google Colab\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üöÄ Setting up Google Colab environment...\")\n",
    "    \n",
    "    # Install required packages\n",
    "    !pip install torch>=2.5.1 torchvision>=0.20.1 -q\n",
    "    !pip install streamlit>=1.41.1 pandas>=2.2.3 numpy>=2.2.2 -q\n",
    "    !pip install scikit-learn>=1.6.1 Pillow==10.0.0 tqdm>=4.67.1 -q\n",
    "    !pip install matplotlib seaborn -q\n",
    "    \n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/thedatadudech/skin-cancer-detection.git\n",
    "    %cd skin-cancer-detection\n",
    "    \n",
    "    print(\"‚úÖ Environment setup complete!\")\n",
    "else:\n",
    "    print(\"üìù Running in local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup for Google Colab\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üìä Setting up dataset for Colab...\")\n",
    "    \n",
    "    # Create data directories\n",
    "    os.makedirs('data/images', exist_ok=True)\n",
    "    \n",
    "    print(\"\"\"üì• Dataset Download Instructions:\n",
    "    \n",
    "    To use this notebook, you need to download the HAM10000 dataset:\n",
    "    \n",
    "    1. Visit: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
    "    2. Download:\n",
    "       - HAM10000_images_part1.zip\n",
    "       - HAM10000_images_part2.zip  \n",
    "       - HAM10000_metadata.csv\n",
    "    3. Upload to Colab and extract:\n",
    "       - Extract zip files to data/images/\n",
    "       - Place metadata.csv in data/\n",
    "    \n",
    "    Or use the sample dataset for demonstration purposes.\"\"\")\n",
    "    \n",
    "    # Option to create sample data for demonstration\n",
    "    create_sample = input(\"Create sample data for demonstration? (y/n): \")\n",
    "    if create_sample.lower() == 'y':\n",
    "        # Create minimal sample dataset for demonstration\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Create sample metadata\n",
    "        sample_data = {\n",
    "            'image_id': [f'sample_{i:03d}' for i in range(50)],\n",
    "            'dx': np.random.choice(['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df'], 50),\n",
    "            'age': np.random.randint(20, 80, 50),\n",
    "            'sex': np.random.choice(['male', 'female'], 50)\n",
    "        }\n",
    "        \n",
    "        df_sample = pd.DataFrame(sample_data)\n",
    "        df_sample.to_csv('data/HAM10000_metadata.csv', index=False)\n",
    "        \n",
    "        # Create sample images\n",
    "        for img_id in sample_data['image_id']:\n",
    "            # Create random RGB image\n",
    "            img_array = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(img_array)\n",
    "            img.save(f'data/images/{img_id}.jpg')\n",
    "        \n",
    "        print(\"‚úÖ Sample dataset created for demonstration\")\n",
    "else:\n",
    "    print(\"üìÇ Using local dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if mounted to load other path\n",
    "if os.path.exists(\"../drive\"):\n",
    "    print(\"Google Drive is mounted.\")\n",
    "    source_path = \"../drive/MyDrive/SkinCancerDetector/\"\n",
    "else:\n",
    "    print(\"Google Drive is not mounted.\")\n",
    "    source_path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore metadata\n",
    "try:\n",
    "    df = pd.read_csv(os.path.join(source_path, 'HAM10000_metadata'))\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Please follow the dataset setup instructions above.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Define class mappings\n",
    "    class_names = {\n",
    "        'akiec': 'Actinic keratoses',\n",
    "        'bcc': 'Basal cell carcinoma', \n",
    "        'bkl': 'Benign keratosis',\n",
    "        'df': 'Dermatofibroma',\n",
    "        'mel': 'Melanoma',\n",
    "        'nv': 'Melanocytic nevi',\n",
    "        'vasc': 'Vascular lesions'\n",
    "    }\n",
    "    \n",
    "    # Display class distribution\n",
    "    print(\"üìä Class Distribution:\")\n",
    "    class_counts = df['dx'].value_counts()\n",
    "    for code, count in class_counts.items():\n",
    "        print(f\"{class_names.get(code, code)}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.countplot(data=df, x='dx', order=class_counts.index)\n",
    "    plt.title('Distribution of Skin Lesion Types in HAM10000 Dataset', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Lesion Type')\n",
    "    plt.ylabel('Number of Images')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(class_counts.values):\n",
    "        ax.text(i, v + 50, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # Update x-axis labels with full names\n",
    "    ax.set_xticklabels([class_names.get(code, code) for code in class_counts.index], \n",
    "                       rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"\\nüìà Dataset Statistics:\")\n",
    "    print(f\"Total images: {len(df):,}\")\n",
    "    print(f\"Number of classes: {df['dx'].nunique()}\")\n",
    "    print(f\"Age range: {df['age'].min():.0f} - {df['age'].max():.0f} years\")\n",
    "    print(f\"Gender distribution: {df['sex'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_properties(data_dir, sample_size=10):\n",
    "    \"\"\"Analyze properties of images in the dataset\"\"\"\n",
    "    image_files = np.random.choice(os.listdir(data_dir), sample_size)\n",
    "    widths, heights = [], []\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img = Image.open(os.path.join(data_dir, img_file))\n",
    "        widths.append(img.size[0])\n",
    "        heights.append(img.size[1])\n",
    "    \n",
    "    return widths, heights\n",
    "\n",
    "widths, heights = analyze_image_properties(os.path.join(source_path, 'images'))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.hist(widths)\n",
    "plt.title('Image Widths Distribution')\n",
    "plt.subplot(122)\n",
    "plt.hist(heights)\n",
    "plt.title('Image Heights Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use project modules if available, otherwise implement inline\n",
    "try:\n",
    "    from src.data_loader import DataLoader as ProjectDataLoader\n",
    "    from src.model import create_model\n",
    "    from src.preprocessing import get_data_transforms\n",
    "    print(\"‚úÖ Using project modules\")\n",
    "    use_project_modules = True\n",
    "except ImportError:\n",
    "    print(\"üìù Project modules not available, using inline implementations\")\n",
    "    use_project_modules = False\n",
    "\n",
    "if df is not None and use_project_modules:\n",
    "    # Use the existing DataLoader from the project\n",
    "    print(\"üîß Setting up data loaders using project DataLoader...\")\n",
    "    \n",
    "    # Initialize the project DataLoader\n",
    "    data_loader = ProjectDataLoader(\n",
    "        data_dir=os.path.join(source_path, 'images'),\n",
    "        metadata_path=os.path.join(source_path, 'HAM10000_metadata'),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = data_loader.create_data_loaders()\n",
    "    \n",
    "    # Load metadata for class mapping\n",
    "    metadata_df = data_loader.load_metadata()\n",
    "    class_names = list(metadata_df['dx'].unique())\n",
    "    \n",
    "    print(f\"\\nüìä Data loaders created successfully:\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    print(f\"\\nüè∑Ô∏è Classes: {class_names}\")\n",
    "    \n",
    "elif df is not None:\n",
    "    print(\"‚ö†Ô∏è Project modules not available, manual data splitting needed\")\n",
    "    # Fallback implementation without project modules\n",
    "    # Create label mapping\n",
    "    label_mapping = {label: idx for idx, label in enumerate(df['dx'].unique())}\n",
    "    print(f\"üè∑Ô∏è Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Add numeric labels to dataframe\n",
    "    df['label'] = df['dx'].map(label_mapping)\n",
    "    \n",
    "    # Split data\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "    \n",
    "    print(f\"\\nüìä Data splits:\")\n",
    "    print(f\"Training: {len(train_df)} images\")\n",
    "    print(f\"Validation: {len(val_df)} images\")\n",
    "    print(f\"Test: {len(test_df)} images\")\n",
    "else:\n",
    "    print(\"‚ùå No dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development and Training\n",
    "\n",
    "We'll implement and compare three different architectures:\n",
    "- **EfficientNet-B0**: State-of-the-art efficient architecture\n",
    "- **ResNet-50**: Popular residual network\n",
    "- **Custom CNN**: Baseline convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architectures\n",
    "def create_efficientnet_model(num_classes=7):\n",
    "    \"\"\"Create EfficientNet-B0 model with transfer learning\"\"\"\n",
    "    model = models.efficientnet_b0(pretrained=True)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def create_resnet_model(num_classes=7):\n",
    "    \"\"\"Create ResNet-50 model with transfer learning\"\"\"\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN for comparison\"\"\"\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Model configurations\n",
    "NUM_CLASSES = 7\n",
    "BATCH_SIZE = 16  # Reduced for better Colab compatibility\n",
    "EPOCHS = 5  # Reduced for demonstration\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "print(f\"üîß Training Configuration:\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instances using project modules or inline implementations\n",
    "if use_project_modules:\n",
    "    # Use project model creation functions\n",
    "    models = {\n",
    "        'EfficientNet': create_model('efficientnet', num_classes=NUM_CLASSES),\n",
    "        'ResNet': create_model('resnet', num_classes=NUM_CLASSES),\n",
    "        'CustomCNN': create_model('custom_cnn', num_classes=NUM_CLASSES)\n",
    "    }\n",
    "    print(\"‚úÖ Models created using project functions\")\n",
    "else:\n",
    "    # Fallback inline model definitions for Colab\n",
    "    def create_efficientnet_model(num_classes=7):\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    def create_resnet_model(num_classes=7):\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    class CustomCNN(nn.Module):\n",
    "        def __init__(self, num_classes=7):\n",
    "            super(CustomCNN, self).__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((7, 7)),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(128 * 7 * 7, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "    models = {\n",
    "        'EfficientNet': create_efficientnet_model(NUM_CLASSES),\n",
    "        'ResNet': create_resnet_model(NUM_CLASSES),\n",
    "        'CustomCNN': CustomCNN(NUM_CLASSES)\n",
    "    }\n",
    "    print(\"‚úÖ Models created using inline implementations\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nüîß Models created:\")\n",
    "for name, model in models.items():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{name}: {total_params:,} total parameters, {trainable_params:,} trainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instances using project modules or inline implementations\n",
    "if use_project_modules:\n",
    "    # Use project model creation functions\n",
    "    models = {\n",
    "        'EfficientNet': create_model('efficientnet', num_classes=NUM_CLASSES),\n",
    "        'ResNet': create_model('resnet', num_classes=NUM_CLASSES),\n",
    "        'CustomCNN': create_model('custom_cnn', num_classes=NUM_CLASSES)\n",
    "    }\n",
    "    print(\"‚úÖ Models created using project functions\")\n",
    "else:\n",
    "    # Fallback inline model definitions for Colab\n",
    "    def create_efficientnet_model(num_classes=7):\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    def create_resnet_model(num_classes=7):\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    class CustomCNN(nn.Module):\n",
    "        def __init__(self, num_classes=7):\n",
    "            super(CustomCNN, self).__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((7, 7)),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(128 * 7 * 7, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "    models = {\n",
    "        'EfficientNet': create_efficientnet_model(NUM_CLASSES),\n",
    "        'ResNet': create_resnet_model(NUM_CLASSES),\n",
    "        'CustomCNN': CustomCNN(NUM_CLASSES)\n",
    "    }\n",
    "    print(\"‚úÖ Models created using inline implementations\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nüîß Models created:\")\n",
    "for name, model in models.items():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{name}: {total_params:,} total parameters, {trainable_params:,} trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "Jetzt trainieren wir unsere PyTorch-Modelle und vergleichen deren Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and store results\n",
    "if use_project_modules and 'train_loader' in locals():\n",
    "    training_histories = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüöÄ Training {name}...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            history = train_pytorch_model(model, train_loader, val_loader, num_epochs=EPOCHS)\n",
    "            training_histories[name] = history\n",
    "            print(f\"‚úÖ {name} training completed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training {name}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "elif not use_project_modules and 'train_df' in locals():\n",
    "    print(\"‚ö†Ô∏è Manual training setup needed without project modules\")\n",
    "    print(\"This would require implementing custom DataLoader for the manual data splits\")\n",
    "    print(\"For best results, use the complete project environment\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot start training: missing data loaders or models\")\n",
    "    print(\"Make sure the project modules are available and data is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for PyTorch\n",
    "def train_pytorch_model(model, train_loader, val_loader, num_epochs=EPOCHS):\n",
    "    \"\"\"Train PyTorch model with proper training loop\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} Training')\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100 * train_correct / train_total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} Validation')\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_train_acc = 100 * train_correct / train_total\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%')\n",
    "        print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save(model.state_dict(), 'best_model_colab.pth')\n",
    "            print(f'New best model saved with validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"‚úÖ PyTorch training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "if 'training_histories' in locals() and training_histories and df is not None:\n",
    "    test_results = {}\n",
    "    \n",
    "    print(\"üß™ Evaluating models on test set...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name not in training_histories:\n",
    "            continue\n",
    "            \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader, desc=f'Testing {name}'):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        \n",
    "        test_results[name] = {\n",
    "            'accuracy': test_accuracy,\n",
    "            'loss': test_loss\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Test Results:\")\n",
    "        print(f\"  Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"  Loss: {test_loss:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Find best model\n",
    "    if test_results:\n",
    "        best_model_name = max(test_results, key=lambda k: test_results[k]['accuracy'])\n",
    "        best_accuracy = test_results[best_model_name]['accuracy']\n",
    "        \n",
    "        print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "        print(f\"üéØ Best Test Accuracy: {best_accuracy:.2f}%\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot evaluate: missing training results or dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and store results\n",
    "if use_project_modules and 'train_loader' in locals():\n",
    "    training_histories = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüöÄ Training {name}...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            history = train_pytorch_model(model, train_loader, val_loader, num_epochs=EPOCHS)\n",
    "            training_histories[name] = history\n",
    "            print(f\"‚úÖ {name} training completed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training {name}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "elif not use_project_modules and 'train_df' in locals():\n",
    "    print(\"‚ö†Ô∏è Manual training setup needed without project modules\")\n",
    "    print(\"This would require implementing custom DataLoader for the manual data splits\")\n",
    "    print(\"For best results, use the complete project environment\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot start training: missing data loaders or models\")\n",
    "    print(\"Make sure the project modules are available and data is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "if 'training_histories' in locals() and training_histories:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(121)\n",
    "    for name, history in training_histories.items():\n",
    "        epochs = range(1, len(history['train_acc']) + 1)\n",
    "        plt.plot(epochs, history['train_acc'], 'o-', label=f'{name} (train)', alpha=0.8)\n",
    "        plt.plot(epochs, history['val_acc'], 's-', label=f'{name} (val)', alpha=0.8)\n",
    "    \n",
    "    plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(122)\n",
    "    for name, history in training_histories.items():\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        plt.plot(epochs, history['train_loss'], 'o-', label=f'{name} (train)', alpha=0.8)\n",
    "        plt.plot(epochs, history['val_loss'], 's-', label=f'{name} (val)', alpha=0.8)\n",
    "    \n",
    "    plt.title('Model Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nüìä Final Training Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, history in training_histories.items():\n",
    "        final_train_acc = history['train_acc'][-1]\n",
    "        final_val_acc = history['val_acc'][-1]\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Final Training Accuracy: {final_train_acc:.2f}%\")\n",
    "        print(f\"  Final Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ùå No training results to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "if 'training_histories' in locals() and training_histories and df is not None:\n",
    "    test_results = {}\n",
    "    \n",
    "    print(\"üß™ Evaluating models on test set...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name not in training_histories:\n",
    "            continue\n",
    "            \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader, desc=f'Testing {name}'):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        \n",
    "        test_results[name] = {\n",
    "            'accuracy': test_accuracy,\n",
    "            'loss': test_loss\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Test Results:\")\n",
    "        print(f\"  Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"  Loss: {test_loss:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Find best model\n",
    "    if test_results:\n",
    "        best_model_name = max(test_results, key=lambda k: test_results[k]['accuracy'])\n",
    "        best_accuracy = test_results[best_model_name]['accuracy']\n",
    "        \n",
    "        print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "        print(f\"üéØ Best Test Accuracy: {best_accuracy:.2f}%\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot evaluate: missing training results or dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Speichern und Zusammenfassung\n",
    "\n",
    "Speichere das beste Modell und erstelle eine Zusammenfassung der Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "if 'test_results' in locals() and test_results:\n",
    "    # Create models directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Save the best model\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    # Save complete model for easy loading\n",
    "    torch.save(best_model, f'models/best_{best_model_name.lower()}_complete.pth')\n",
    "    \n",
    "    # Save only state dict (smaller file)\n",
    "    torch.save(best_model.state_dict(), f'models/best_{best_model_name.lower()}_weights.pth')\n",
    "    \n",
    "    print(f\"‚úÖ Best model ({best_model_name}) saved successfully!\")\n",
    "    print(f\"üìÅ Complete model: models/best_{best_model_name.lower()}_complete.pth\")\n",
    "    print(f\"üìÅ Model weights: models/best_{best_model_name.lower()}_weights.pth\")\n",
    "    print(f\"üéØ Test Accuracy: {best_accuracy:.2f}%\")\n",
    "    \n",
    "    # Create model summary\n",
    "    print(\"\\nüìã Training Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Dataset: HAM10000 (Skin Cancer Detection)\")\n",
    "    print(f\"Classes: {NUM_CLASSES} skin lesion types\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"Best accuracy: {best_accuracy:.2f}%\")\n",
    "    print(f\"Training epochs: {EPOCHS}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Device used: {device}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot save model: no test results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Zusammenfassung\n",
    "\n",
    "Dieses Notebook demonstriert eine vollst√§ndige PyTorch-basierte Pipeline f√ºr die Hautkrebs-Erkennung:\n",
    "\n",
    "### Was wir erreicht haben:\n",
    "- **Datenanalyse**: Exploration des HAM10000 Datensatzes\n",
    "- **Modellvergleich**: EfficientNet, ResNet und Custom CNN\n",
    "- **Training**: Vollst√§ndige PyTorch Trainings-Pipeline\n",
    "- **Evaluation**: Comprehensive Leistungsbewertung\n",
    "- **Deployment**: Modell f√ºr Produktivumgebung bereit\n",
    "\n",
    "### Medizinischer Kontext:\n",
    "- 7 verschiedene Hautl√§sionstypen klassifiziert\n",
    "- Unterst√ºtzt medizinisches Fachpersonal bei der Voruntersuchung\n",
    "- Wichtig: Nur f√ºr Bildungs- und Forschungszwecke\n",
    "\n",
    "### N√§chste Schritte:\n",
    "1. Modell in Streamlit App integrieren\n",
    "2. Weitere Datenaugmentation testen\n",
    "3. Hyperparameter-Optimierung\n",
    "4. Ensemble-Methoden evaluieren\n",
    "\n",
    "**‚ö†Ô∏è Medizinischer Hinweis**: Dieses Tool dient nur zu Bildungs- und Forschungszwecken. Konsultieren Sie immer qualifizierte medizinische Fachkr√§fte f√ºr medizinische Diagnosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
