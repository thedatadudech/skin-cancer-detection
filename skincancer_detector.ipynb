{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skin-cancer-detection-header"
   },
   "source": [
    "# üî¨ Skin Cancer Detection System - Complete Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thedatadudech/skin-cancer-detection/blob/main/skincancer_detector.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/thedatadudech/skin-cancer-detection)\n",
    "\n",
    "This comprehensive notebook demonstrates the complete development process of an advanced skin cancer detection system using deep learning with PyTorch. \n",
    "\n",
    "## üìã What You'll Learn\n",
    "- **Data Analysis**: Explore the HAM10000 medical imaging dataset\n",
    "- **Model Development**: Compare EfficientNet, ResNet, and custom CNN architectures\n",
    "- **Training Pipeline**: Implement robust training with data augmentation\n",
    "- **Performance Evaluation**: Analyze model performance across 7 skin lesion types\n",
    "- **Deployment Ready**: Export models for production use\n",
    "\n",
    "## üéØ Medical Context\n",
    "Early detection of skin cancer, particularly melanoma, is crucial for successful treatment. This system assists healthcare professionals in preliminary screening by analyzing dermoscopic images.\n",
    "\n",
    "**‚ö†Ô∏è Medical Disclaimer**: This tool is for educational and research purposes only. Always consult qualified healthcare professionals for medical diagnosis.\n",
    "\n",
    "## üöÄ Quick Start\n",
    "1. Click the \"Open in Colab\" button above\n",
    "2. Run the setup cells to install dependencies\n",
    "3. Download the HAM10000 dataset (instructions below)\n",
    "4. Execute the training pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-setup"
   },
   "source": [
    "## üîß Google Colab Setup\n",
    "\n",
    "If running on Google Colab, execute the following cells to set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab-install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install dependencies for Google Colab\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üöÄ Setting up Google Colab environment...\")\n",
    "    \n",
    "    # Install required packages\n",
    "    !pip install torch>=2.5.1 torchvision>=0.20.1 -q\n",
    "    !pip install streamlit>=1.41.1 pandas>=2.2.3 numpy>=2.2.2 -q\n",
    "    !pip install scikit-learn>=1.6.1 Pillow==10.0.0 tqdm>=4.67.1 -q\n",
    "    !pip install matplotlib seaborn -q\n",
    "    \n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/thedatadudech/skin-cancer-detection.git\n",
    "    %cd skin-cancer-detection\n",
    "    \n",
    "    print(\"‚úÖ Environment setup complete!\")\n",
    "else:\n",
    "    print(\"üìù Running in local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab-dataset-setup"
   },
   "outputs": [],
   "source": [
    "# Dataset setup for Google Colab\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üìä Setting up dataset for Colab...\")\n",
    "    \n",
    "    # Create data directories\n",
    "    os.makedirs('data/images', exist_ok=True)\n",
    "    \n",
    "    print(\"\"\"üì• Dataset Download Instructions:\n",
    "    \n",
    "    To use this notebook, you need to download the HAM10000 dataset:\n",
    "    \n",
    "    1. Visit: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
    "    2. Download:\n",
    "       - HAM10000_images_part1.zip\n",
    "       - HAM10000_images_part2.zip  \n",
    "       - HAM10000_metadata.csv\n",
    "    3. Upload to Colab and extract:\n",
    "       - Extract zip files to data/images/\n",
    "       - Place metadata.csv in data/\n",
    "    \n",
    "    Or use the sample dataset for demonstration purposes.\"\"\")\n",
    "    \n",
    "    # Option to create sample data for demonstration\n",
    "    create_sample = input(\"Create sample data for demonstration? (y/n): \")\n",
    "    if create_sample.lower() == 'y':\n",
    "        # Create minimal sample dataset for demonstration\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Create sample metadata\n",
    "        sample_data = {\n",
    "            'image_id': [f'sample_{i:03d}' for i in range(50)],\n",
    "            'dx': np.random.choice(['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df'], 50),\n",
    "            'age': np.random.randint(20, 80, 50),\n",
    "            'sex': np.random.choice(['male', 'female'], 50)\n",
    "        }\n",
    "        \n",
    "        df_sample = pd.DataFrame(sample_data)\n",
    "        df_sample.to_csv('data/HAM10000_metadata.csv', index=False)\n",
    "        \n",
    "        # Create sample images\n",
    "        for img_id in sample_data['image_id']:\n",
    "            # Create random RGB image\n",
    "            img_array = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(img_array)\n",
    "            img.save(f'data/images/{img_id}.jpg')\n",
    "        \n",
    "        print(\"‚úÖ Sample dataset created for demonstration\")\n",
    "else:\n",
    "    print(\"üìÇ Using local dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-metadata"
   },
   "outputs": [],
   "source": [
    "# Load and explore metadata\n",
    "try:\n",
    "    df = pd.read_csv('data/HAM10000_metadata.csv')\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Please follow the dataset setup instructions above.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze-classes"
   },
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Define class mappings\n",
    "    class_names = {\n",
    "        'akiec': 'Actinic keratoses',\n",
    "        'bcc': 'Basal cell carcinoma', \n",
    "        'bkl': 'Benign keratosis',\n",
    "        'df': 'Dermatofibroma',\n",
    "        'mel': 'Melanoma',\n",
    "        'nv': 'Melanocytic nevi',\n",
    "        'vasc': 'Vascular lesions'\n",
    "    }\n",
    "    \n",
    "    # Display class distribution\n",
    "    print(\"üìä Class Distribution:\")\n",
    "    class_counts = df['dx'].value_counts()\n",
    "    for code, count in class_counts.items():\n",
    "        print(f\"{class_names.get(code, code)}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.countplot(data=df, x='dx', order=class_counts.index)\n",
    "    plt.title('Distribution of Skin Lesion Types in HAM10000 Dataset', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Lesion Type')\n",
    "    plt.ylabel('Number of Images')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(class_counts.values):\n",
    "        ax.text(i, v + 50, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # Update x-axis labels with full names\n",
    "    ax.set_xticklabels([class_names.get(code, code) for code in class_counts.index], \n",
    "                       rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"\\nüìà Dataset Statistics:\")\n",
    "    print(f\"Total images: {len(df):,}\")\n",
    "    print(f\"Number of classes: {df['dx'].nunique()}\")\n",
    "    print(f\"Age range: {df['age'].min():.0f} - {df['age'].max():.0f} years\")\n",
    "    print(f\"Gender distribution: {df['sex'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_image_properties(data_dir, sample_size=100):\n",
    "    \"\"\"Analyze properties of images in the dataset\"\"\"\n",
    "    image_files = np.random.choice(os.listdir(data_dir), sample_size)\n",
    "    widths, heights = [], []\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img = Image.open(os.path.join(data_dir, img_file))\n",
    "        widths.append(img.size[0])\n",
    "        heights.append(img.size[1])\n",
    "    \n",
    "    return widths, heights\n",
    "\n",
    "widths, heights = analyze_image_properties('data/images')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.hist(widths)\n",
    "plt.title('Image Widths Distribution')\n",
    "plt.subplot(122)\n",
    "plt.hist(heights)\n",
    "plt.title('Image Heights Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-data-loaders"
   },
   "outputs": [],
   "source": [
    "# Use project modules if available, otherwise implement inline\n",
    "try:\n",
    "    from src.data_loader import DataLoader\n",
    "    from src.model import create_model\n",
    "    from src.preprocessing import get_data_transforms\n",
    "    print(\"‚úÖ Using project modules\")\n",
    "    use_project_modules = True\n",
    "except ImportError:\n",
    "    print(\"üìù Using inline implementations for Colab compatibility\")\n",
    "    use_project_modules = False\n",
    "\n",
    "if df is not None:\n",
    "    # Create label mapping\n",
    "    label_mapping = {label: idx for idx, label in enumerate(df['dx'].unique())}\n",
    "    print(f\"\\nüè∑Ô∏è Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Add numeric labels to dataframe\n",
    "    df['label'] = df['dx'].map(label_mapping)\n",
    "    \n",
    "    # Split data\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "    \n",
    "    print(f\"\\nüìä Data splits:\")\n",
    "    print(f\"Training: {len(train_df)} images\")\n",
    "    print(f\"Validation: {len(val_df)} images\")\n",
    "    print(f\"Test: {len(test_df)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-development-header"
   },
   "source": [
    "## 4. Model Development and Training\n",
    "\n",
    "We'll implement and compare three different architectures:\n",
    "- **EfficientNet-B0**: State-of-the-art efficient architecture\n",
    "- **ResNet-50**: Popular residual network\n",
    "- **Custom CNN**: Baseline convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-implementations"
   },
   "outputs": [],
   "source": [
    "# Define model architectures\n",
    "def create_efficientnet_model(num_classes=7):\n",
    "    \"\"\"Create EfficientNet-B0 model with transfer learning\"\"\"\n",
    "    model = models.efficientnet_b0(pretrained=True)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def create_resnet_model(num_classes=7):\n",
    "    \"\"\"Create ResNet-50 model with transfer learning\"\"\"\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN for comparison\"\"\"\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Model configurations\n",
    "NUM_CLASSES = 7\n",
    "BATCH_SIZE = 16  # Reduced for better Colab compatibility\n",
    "EPOCHS = 5  # Reduced for demonstration\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "print(f\"üîß Training Configuration:\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "data_augmentation = create_data_augmentation()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3\n",
    "    )\n",
    "]\n",
    "\n",
    "# Dictionary to store training histories\n",
    "histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        data_augmentation(X_train),\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    histories[name] = history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training histories\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "for name, history in histories.items():\n",
    "    plt.plot(history['accuracy'], label=f'{name} (train)')\n",
    "    plt.plot(history['val_accuracy'], label=f'{name} (val)')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "for name, history in histories.items():\n",
    "    plt.plot(history['loss'], label=f'{name} (train)')\n",
    "    plt.plot(history['val_loss'], label=f'{name} (val)')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate models on test set\n",
    "test_results = {}\n",
    "for name, model in models.items():\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_results[name] = {\n",
    "        'accuracy': test_acc,\n",
    "        'loss': test_loss\n",
    "    }\n",
    "    print(f\"\\n{name} Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find and save the best model\n",
    "best_model_name = max(test_results, key=lambda k: test_results[k]['accuracy'])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "best_model.save('models/best_model.h5')\n",
    "print(f\"Best model ({best_model_name}) saved with test accuracy: {test_results[best_model_name]['accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
